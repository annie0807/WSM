{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc048e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from Parser import Parser\n",
    "import util\n",
    "from tfidf import *\n",
    "import glob, os\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "from __future__ import division, unicode_literals\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "class VectorSpace:\n",
    "    \"\"\" A algebraic model for representing text documents as vectors of identifiers. \n",
    "    A document is represented as a vector. Each dimension of the vector corresponds to a \n",
    "    separate term. If a term occurs in the document, then the value in the vector is non-zero.\n",
    "    \"\"\"\n",
    "\n",
    "    #Collection of document term vectors\n",
    "    documentVectors = []\n",
    "\n",
    "    #Mapping of vector index to keyword\n",
    "    vectorKeywordIndex=[]\n",
    "\n",
    "    #Tidies terms\n",
    "    parser=None\n",
    "    \n",
    "    def __init__(self, documents=[], vectorMode = 'tf'):\n",
    "        self.documentVectors=[]\n",
    "        self.parser = Parser()\n",
    "        self.BlobList = self.getBlobList(documents)\n",
    "        self.vectorMode = vectorMode\n",
    "        if(len(documents)>0):\n",
    "            self.build(documents)\n",
    "    \n",
    "    def getBlobList(self, documents): \n",
    "        bloblist = []\n",
    "        for doc in documents:\n",
    "            wordList = self.parser.tokenise(doc)\n",
    "            wordList = self.parser.removeStopWords(wordList)\n",
    "            bloblist.append(tb(\" \".join(wordList)))\n",
    "        return bloblist\n",
    "            \n",
    "    def build(self,documents):\n",
    "        \"\"\" Create the vector space for the passed document strings \"\"\"\n",
    "        self.vectorKeywordIndex = self.getVectorKeywordIndex(documents)\n",
    "        self.documentVectors = [self.makeVector(document, self.vectorMode) for document in tqdm(documents)]\n",
    "\n",
    "    def getVectorKeywordIndex(self, documentList):\n",
    "        \"\"\" create the keyword associated to the position of the elements within the document vectors \"\"\"\n",
    "\n",
    "        #Mapped documents into a single word string\t\n",
    "        vocabularyString = \" \".join(documentList)\n",
    "\n",
    "        vocabularyList = self.parser.tokenise(vocabularyString) #斷詞\n",
    "        #Remove common words which have no search value\n",
    "        vocabularyList = self.parser.removeStopWords(vocabularyList) # 去掉stop words\n",
    "        uniqueVocabularyList = util.removeDuplicates(vocabularyList) # 去掉重複詞\n",
    "        \n",
    "        vectorIndex={}\n",
    "        offset=0\n",
    "        #Associate a position with the keywords which maps to the dimension on the vector used to represent this word\n",
    "        for word in uniqueVocabularyList:\n",
    "            vectorIndex[word]=offset\n",
    "            offset+=1\n",
    "        return vectorIndex  #(keyword:position)\n",
    "\n",
    "\n",
    "    def makeVector(self, wordString, mode):\n",
    "        \"\"\" @pre: unique(vectorIndex) \"\"\"\n",
    "        #Initialise vector with 0's\n",
    "        vector = [0] * len(self.vectorKeywordIndex)\n",
    "        wordList = self.parser.tokenise(wordString)\n",
    "        wordList = self.parser.removeStopWords(wordList)\n",
    "        tbString = tb(\" \".join(wordList))\n",
    "        if mode == 'tf':\n",
    "            for word in list(set(wordList)):\n",
    "                vector[self.vectorKeywordIndex[word]] = tf(word, tbString) #Use simple Term Count Model\n",
    "            return vector \n",
    "        \n",
    "        if mode == 'tf-idf':\n",
    "            #print('bloblist:', self.BlobList)\n",
    "            for word in list(set(wordList)):\n",
    "                vector[self.vectorKeywordIndex[word]] =  tfidf(word, tbString , self.BlobList) \n",
    "            return vector\n",
    "        \n",
    "    def buildQueryVector(self, termList):\n",
    "        \"\"\" convert query string into a term vector \"\"\"\n",
    "        query = self.makeVector(\" \".join(termList), self.vectorMode)\n",
    "        return query\n",
    "\n",
    "    def related(self,documentId):\n",
    "        \"\"\" find documents that are related to the document indexed by passed Id within the document Vectors\"\"\"\n",
    "        ratings = [util.cosine(self.documentVectors[documentId], documentVector) for documentVector in self.documentVectors]\n",
    "        #ratings.sort(reverse=True)\n",
    "        return ratings\n",
    "    \n",
    "    def search(self,searchList, mode = 'cos'):\n",
    "        \"\"\" search for documents that match based on a list of terms \"\"\"\n",
    "        if type(searchList[0]) == str:\n",
    "            queryVector = self.buildQueryVector(searchList)\n",
    "        else:\n",
    "            queryVector = searchList\n",
    "        \n",
    "        if mode == 'cos':\n",
    "            ratings = [util.cosine(queryVector, documentVector) for documentVector in tqdm(self.documentVectors)]\n",
    "        #ratings.sort(reverse=True)\n",
    "            return ratings\n",
    "        if mode == 'eucli':\n",
    "            ratings = [util.Euclidean(queryVector, documentVector) for documentVector in tqdm(self.documentVectors)]\n",
    "            return ratings\n",
    "\n",
    "    def printresult(self,searchlist, files,n,mode='cos'):\n",
    "        scoreList = self.search(searchlist, mode = mode)\n",
    "        for i in np.flip(np.argsort(scoreList))[:n]:\n",
    "            print( 'document:' , files[i],'score:', scoreList[i])\n",
    "        return np.flip(np.argsort(scoreList))[:n]\n",
    "            \n",
    "    def getFeedbackVector(self, searchList, top10results ,mode = 'cos'):\n",
    "        queryVector = self.buildQueryVector(searchList)\n",
    "        feedbackVector = self.buildQueryVector(top10results)\n",
    "        queryArray = np.array(queryVector)\n",
    "        feedbackArray = np.array(feedbackVector)\n",
    "        newQueryVector = list(queryArray + 0.5 * feedbackArray)\n",
    "        return newQueryVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919b5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "documents_chi = [] #0-499是英文檔案 ， 500-1500是中文檔案\n",
    "files_chi = []\n",
    "for file in os.listdir(\"./News/News\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filename_chi = os.path.join(\"./News/News\", file)\n",
    "        files_chi.append(file[:-4])\n",
    "        with open(filename_chi, encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            doc = ' '.join(lines)\n",
    "            doc1 = doc.replace(\"\\n\", \"\")\n",
    "            documents_chi.append(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c92602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "documents_chi = [] #0-499是英文檔案 ， 500-1500是中文檔案\n",
    "files_chi = []\n",
    "for file in os.listdir(\"./News/News\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filename_chi = os.path.join(\"./News/News\", file)\n",
    "        files_chi.append(file[:-4])\n",
    "        with open(filename_chi, encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            doc = ' '.join(lines)\n",
    "            doc1 = doc.replace(\"\\n\", \"\")\n",
    "            documents_chi.append(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8722b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(documents_chi[500:1500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22152a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: nan document: I haven't got a hat.\n",
      "score: nan document: Dogs and cats make good pets.\n",
      "score: nan document: A cat is a fine pet ponies.\n",
      "score: nan document: The cat cat in the hat disabled\n",
      "score: 1.2345525572306575 document: Dogs and cats make good pets.\n",
      "score: 1.0216002166437488 document: A cat is a fine pet ponies.\n",
      "score: 0.7504758415354574 document: The cat cat in the hat disabled\n",
      "score: 0.28768207245178085 document: I haven't got a hat.\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "\n",
    "documents = [\"The cat cat in the hat disabled\",\n",
    "                 \"A cat is a fine pet ponies.\",\n",
    "                 \"Dogs and cats make good pets.\",\n",
    "                 \"I haven't got a hat.\",\n",
    "            \"烏克蘭總統是澤倫斯基。\", \n",
    "            \"澤倫斯基是親西方國家的烏克蘭總統，這讓普丁很不爽\"]\n",
    "\n",
    "vectorSpace = VectorSpace(documents, 'tf-idf')  # vectorSpace(documents, vectorMode = 'tf' or 'tf-idf') (default is 'tf')\n",
    "\n",
    "\n",
    "#print(vectorSpace.vectorKeywordIndex)\n",
    "\n",
    "#print(vectorSpace.documentVectors)\n",
    "\n",
    "#print(vectorSpace.related(1))\n",
    "\n",
    "#print(vectorSpace.search([\"cat\"]))   \n",
    "\n",
    "vectorSpace.printresult([\"cat\"])\n",
    "vectorSpace.printresult([\"cat\"], mode = 'eucli')  # mode = 'cos' or 'eucli' (default is 'cos')\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179dc26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

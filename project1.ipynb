{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efc048e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from Parser import Parser\n",
    "import util\n",
    "import tfidf\n",
    "import glob, os\n",
    "from __future__ import division, unicode_literals\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "class VectorSpace:\n",
    "    \"\"\" A algebraic model for representing text documents as vectors of identifiers. \n",
    "    A document is represented as a vector. Each dimension of the vector corresponds to a \n",
    "    separate term. If a term occurs in the document, then the value in the vector is non-zero.\n",
    "    \"\"\"\n",
    "\n",
    "    #Collection of document term vectors\n",
    "    documentVectors = []\n",
    "\n",
    "    #Mapping of vector index to keyword\n",
    "    vectorKeywordIndex=[]\n",
    "\n",
    "    #Tidies terms\n",
    "    parser=None\n",
    "    \n",
    "    def __init__(self, documents=[]):\n",
    "        self.documentVectors=[]\n",
    "        self.parser = Parser()\n",
    "        self.BlobList = self.getBlobList(documents)\n",
    "        \n",
    "        if(len(documents)>0):\n",
    "            self.build(documents)\n",
    "    \n",
    "    def getBlobList(self, documents): \n",
    "        bloblist = []\n",
    "        for doc in documents:\n",
    "            wordList = self.parser.tokenise(doc)\n",
    "            wordList = self.parser.removeStopWords(wordList)\n",
    "            bloblist.append(tb(\" \".join(wordList)))\n",
    "        return bloblist\n",
    "            \n",
    "    def build(self,documents):\n",
    "        \"\"\" Create the vector space for the passed document strings \"\"\"\n",
    "        self.vectorKeywordIndex = self.getVectorKeywordIndex(documents)\n",
    "        self.documentVectors = [self.makeVector(document) for document in documents]\n",
    "\n",
    "        #print(self.vectorKeywordIndex)\n",
    "        #print(self.documentVectors)\n",
    "\n",
    "\n",
    "    def getVectorKeywordIndex(self, documentList):\n",
    "        \"\"\" create the keyword associated to the position of the elements within the document vectors \"\"\"\n",
    "\n",
    "        #Mapped documents into a single word string\t\n",
    "        vocabularyString = \" \".join(documentList)\n",
    "\n",
    "        vocabularyList = self.parser.tokenise(vocabularyString)\n",
    "        #Remove common words which have no search value\n",
    "        vocabularyList = self.parser.removeStopWords(vocabularyList)\n",
    "        uniqueVocabularyList = util.removeDuplicates(vocabularyList)\n",
    "\n",
    "        vectorIndex={}\n",
    "        offset=0\n",
    "        #Associate a position with the keywords which maps to the dimension on the vector used to represent this word\n",
    "        for word in uniqueVocabularyList:\n",
    "            vectorIndex[word]=offset\n",
    "            offset+=1\n",
    "        return vectorIndex  #(keyword:position)\n",
    "\n",
    "\n",
    "    def makeVector(self, wordString, mode = 'tf-idf'):\n",
    "        \"\"\" @pre: unique(vectorIndex) \"\"\"\n",
    "        #Initialise vector with 0's\n",
    "        vector = [0] * len(self.vectorKeywordIndex)\n",
    "        wordList = self.parser.tokenise(wordString)\n",
    "        wordList = self.parser.removeStopWords(wordList)\n",
    "        tbString = tb(\" \".join(wordList))\n",
    "        if mode == 'tf':\n",
    "            for word in list(set(wordList)):\n",
    "                vector[self.vectorKeywordIndex[word]] = self.tf(word, tbString) #Use simple Term Count Model\n",
    "            return vector \n",
    "        \n",
    "        if mode == 'tf-idf':\n",
    "            print('bloblist:', self.BlobList)\n",
    "            for word in list(set(wordList)):\n",
    "                vector[self.vectorKeywordIndex[word]] =  self.idf(word, self.BlobList) \n",
    "                print('word:',word, 'idf:',self.idf(word, self.BlobList),  )\n",
    "            return vector\n",
    "            \n",
    "    \n",
    "    def tf(self, word, blob):\n",
    "        return blob.words.count(word) / len(blob.words)\n",
    "    \n",
    "    def n_containing(self, word, bloblist):\n",
    "        print()\n",
    "        return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "    def idf(self, word, bloblist):\n",
    "        print('總文件數量：', len(bloblist), '含有{}字的文章數量+1:'.format(word), (1 + self.n_containing(word, bloblist)))\n",
    "        print('{}的idf為:'.format(word), math.log(len(bloblist) / (1 + self.n_containing(word, bloblist))))\n",
    "        return math.log(len(bloblist) / (1 + self.n_containing(word, bloblist)))\n",
    "\n",
    "    def tfidf(self, word, blob, bloblist):\n",
    "        return tf(word, blob) * idf(word, bloblist)\n",
    "\n",
    "    def old_makeVector(self, wordString):\n",
    "        \"\"\" @pre: unique(vectorIndex) \"\"\"\n",
    "\n",
    "        #Initialise vector with 0's\n",
    "        vector = [0] * len(self.vectorKeywordIndex)\n",
    "        wordList = self.parser.tokenise(wordString)\n",
    "        wordList = self.parser.removeStopWords(wordList)\n",
    "        for word in wordList:\n",
    "            vector[self.vectorKeywordIndex[word]] += 1; #Use simple Term Count Model\n",
    "        return vector\n",
    "\n",
    "\n",
    "    def buildQueryVector(self, termList):\n",
    "        \"\"\" convert query string into a term vector \"\"\"\n",
    "        query = self.makeVector(\" \".join(termList))\n",
    "        return query\n",
    "\n",
    "\n",
    "    def related(self,documentId):\n",
    "        \"\"\" find documents that are related to the document indexed by passed Id within the document Vectors\"\"\"\n",
    "        ratings = [util.cosine(self.documentVectors[documentId], documentVector) for documentVector in self.documentVectors]\n",
    "        #ratings.sort(reverse=True)\n",
    "        return ratings\n",
    "\n",
    "\n",
    "    def search(self,searchList):\n",
    "        \"\"\" search for documents that match based on a list of terms \"\"\"\n",
    "        queryVector = self.buildQueryVector(searchList)\n",
    "        #print(queryVector)\n",
    "        ratings = [util.cosine(queryVector, documentVector) for documentVector in self.documentVectors]\n",
    "        #ratings.sort(reverse=True)\n",
    "        return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22152a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloblist: [TextBlob(\"cat cat hat disabl\"), TextBlob(\"cat fine pet poni\"), TextBlob(\"dog cat make good pet\"), TextBlob(\"hat\")]\n",
      "\n",
      "總文件數量： 4 含有cat字的文章數量+1: 4\n",
      "\n",
      "cat的idf為: 0.0\n",
      "\n",
      "\n",
      "總文件數量： 4 含有cat字的文章數量+1: 4\n",
      "\n",
      "cat的idf為: 0.0\n",
      "\n",
      "word: cat idf: 0.0\n",
      "\n",
      "總文件數量： 4 含有hat字的文章數量+1: 3\n",
      "\n",
      "hat的idf為: 0.28768207245178085\n",
      "\n",
      "\n",
      "總文件數量： 4 含有hat字的文章數量+1: 3\n",
      "\n",
      "hat的idf為: 0.28768207245178085\n",
      "\n",
      "word: hat idf: 0.28768207245178085\n",
      "\n",
      "總文件數量： 4 含有disabl字的文章數量+1: 2\n",
      "\n",
      "disabl的idf為: 0.6931471805599453\n",
      "\n",
      "\n",
      "總文件數量： 4 含有disabl字的文章數量+1: 2\n",
      "\n",
      "disabl的idf為: 0.6931471805599453\n",
      "\n",
      "word: disabl idf: 0.6931471805599453\n",
      "bloblist: [TextBlob(\"cat cat hat disabl\"), TextBlob(\"cat fine pet poni\"), TextBlob(\"dog cat make good pet\"), TextBlob(\"hat\")]\n",
      "\n",
      "總文件數量： 4 含有cat字的文章數量+1: 4\n",
      "\n",
      "cat的idf為: 0.0\n",
      "\n",
      "\n",
      "總文件數量： 4 含有cat字的文章數量+1: 4\n",
      "\n",
      "cat的idf為: 0.0\n",
      "\n",
      "word: cat idf: 0.0\n",
      "\n",
      "總文件數量： 4 含有fine字的文章數量+1: 2\n",
      "\n",
      "fine的idf為: 0.6931471805599453\n",
      "\n",
      "\n",
      "總文件數量： 4 含有fine字的文章數量+1: 2\n",
      "\n",
      "fine的idf為: 0.6931471805599453\n",
      "\n",
      "word: fine idf: 0.6931471805599453\n",
      "\n",
      "總文件數量： 4 含有pet字的文章數量+1: 3\n",
      "\n",
      "pet的idf為: 0.28768207245178085\n",
      "\n",
      "\n",
      "總文件數量： 4 含有pet字的文章數量+1: 3\n",
      "\n",
      "pet的idf為: 0.28768207245178085\n",
      "\n",
      "word: pet idf: 0.28768207245178085\n",
      "\n",
      "總文件數量： 4 含有poni字的文章數量+1: 2\n",
      "\n",
      "poni的idf為: 0.6931471805599453\n",
      "\n",
      "\n",
      "總文件數量： 4 含有poni字的文章數量+1: 2\n",
      "\n",
      "poni的idf為: 0.6931471805599453\n",
      "\n",
      "word: poni idf: 0.6931471805599453\n",
      "bloblist: [TextBlob(\"cat cat hat disabl\"), TextBlob(\"cat fine pet poni\"), TextBlob(\"dog cat make good pet\"), TextBlob(\"hat\")]\n",
      "\n",
      "總文件數量： 4 含有dog字的文章數量+1: 2\n",
      "\n",
      "dog的idf為: 0.6931471805599453\n",
      "\n",
      "\n",
      "總文件數量： 4 含有dog字的文章數量+1: 2\n",
      "\n",
      "dog的idf為: 0.6931471805599453\n",
      "\n",
      "word: dog idf: 0.6931471805599453\n",
      "\n",
      "總文件數量： 4 含有make字的文章數量+1: 2\n",
      "\n",
      "make的idf為: 0.6931471805599453\n",
      "\n",
      "\n",
      "總文件數量： 4 含有make字的文章數量+1: 2\n",
      "\n",
      "make的idf為: 0.6931471805599453\n",
      "\n",
      "word: make idf: 0.6931471805599453\n",
      "\n",
      "總文件數量： 4 含有pet字的文章數量+1: 3\n",
      "\n",
      "pet的idf為: 0.28768207245178085\n",
      "\n",
      "\n",
      "總文件數量： 4 含有pet字的文章數量+1: 3\n",
      "\n",
      "pet的idf為: 0.28768207245178085\n",
      "\n",
      "word: pet idf: 0.28768207245178085\n",
      "\n",
      "總文件數量： 4 含有good字的文章數量+1: 2\n",
      "\n",
      "good的idf為: 0.6931471805599453\n",
      "\n",
      "\n",
      "總文件數量： 4 含有good字的文章數量+1: 2\n",
      "\n",
      "good的idf為: 0.6931471805599453\n",
      "\n",
      "word: good idf: 0.6931471805599453\n",
      "\n",
      "總文件數量： 4 含有cat字的文章數量+1: 4\n",
      "\n",
      "cat的idf為: 0.0\n",
      "\n",
      "\n",
      "總文件數量： 4 含有cat字的文章數量+1: 4\n",
      "\n",
      "cat的idf為: 0.0\n",
      "\n",
      "word: cat idf: 0.0\n",
      "bloblist: [TextBlob(\"cat cat hat disabl\"), TextBlob(\"cat fine pet poni\"), TextBlob(\"dog cat make good pet\"), TextBlob(\"hat\")]\n",
      "\n",
      "總文件數量： 4 含有hat字的文章數量+1: 3\n",
      "\n",
      "hat的idf為: 0.28768207245178085\n",
      "\n",
      "\n",
      "總文件數量： 4 含有hat字的文章數量+1: 3\n",
      "\n",
      "hat的idf為: 0.28768207245178085\n",
      "\n",
      "word: hat idf: 0.28768207245178085\n",
      "[[0.28768207245178085, 0, 0, 0, 0, 0, 0, 0.6931471805599453, 0.0], [0, 0.6931471805599453, 0, 0, 0.28768207245178085, 0.6931471805599453, 0, 0, 0.0], [0, 0, 0.6931471805599453, 0.6931471805599453, 0.28768207245178085, 0, 0.6931471805599453, 0, 0.0], [0.28768207245178085, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "\n",
    "documents = [\"The cat cat in the hat disabled\",\n",
    "                 \"A cat is a fine pet ponies.\",\n",
    "                 \"Dogs and cats make good pets.\",\n",
    "                 \"I haven't got a hat.\"]\n",
    "\n",
    "vectorSpace = VectorSpace(documents)\n",
    "\n",
    "#print(vectorSpace.vectorKeywordIndex)\n",
    "\n",
    "print(vectorSpace.documentVectors)\n",
    "\n",
    "#print(vectorSpace.related(1))\n",
    "\n",
    "#print(vectorSpace.search([\"cat\"]))\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6de1076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1249387366082999"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(4/3) /math.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21bd51c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump aide who stormed the Capitol broke an 'oath to protect America,' judge says WASHINGTON (Reuters) - A man appointed to the U.S. State Department during the Trump administration will remain in jail while he awaits trial on charges that he took part in the deadly storming of the U.S. Capitol and assaulted police officers, a judge said on Tuesday.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndocuments = []\\nbloblist = []\\nfor file in os.listdir(\"./EnglishNews/EnglishNews\"):\\n    if file.endswith(\".txt\"):\\n        filename = os.path.join(\"./EnglishNews/EnglishNews\", file)\\n        bloblist.append(filename)\\n        with open(filename, encoding=\"utf-8\") as f:\\n            lines = f.readlines()\\n            doc = \\' \\'.join(lines)\\n            doc1 = doc.replace(\"\\n\", \"\")\\n            documents.append(doc1)\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./EnglishNews/EnglishNews/News100012.txt\", encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    doc = ' '.join(lines)\n",
    "    doc1 = doc.replace(\"\\n\", \"\")\n",
    "\n",
    "print(doc1)\n",
    "\n",
    "\n",
    "'''\n",
    "documents = []\n",
    "bloblist = []\n",
    "for file in os.listdir(\"./EnglishNews/EnglishNews\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filename = os.path.join(\"./EnglishNews/EnglishNews\", file)\n",
    "        bloblist.append(filename)\n",
    "        with open(filename, encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            doc = ' '.join(lines)\n",
    "            doc1 = doc.replace(\"\\n\", \"\")\n",
    "            documents.append(doc1)\n",
    "'''             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5fe3cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump aide who stormed the Capitol broke an 'oath to protect America,' judge says WASHINGTON (Reuters) - A man appointed to the U.S. State Department during the Trump administration will remain in jail while he awaits trial on charges that he took part in the deadly storming of the U.S. Capitol and assaulted police officers, a judge said on Tuesday. China\n"
     ]
    }
   ],
   "source": [
    "query = tb(\"Trump Biden Taiwan China\")\n",
    "document0 = tb(documents[0])\n",
    "for word in query.words:\n",
    "    if word not in document0.words:\n",
    "        vector = document0 + ' '+ word\n",
    "print(vector)\n",
    "#doc_que = \" \".join(document0 + query)\n",
    "#print(doc_que)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9143e078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: python, TF-IDF: 0.01662\n",
      "\tWord: films, TF-IDF: 0.00997\n",
      "\tWord: made-for-TV, TF-IDF: 0.00665\n",
      "Top words in document 2\n",
      "\tWord: genus, TF-IDF: 0.02253\n",
      "\tWord: 2, TF-IDF: 0.02253\n",
      "\tWord: from, TF-IDF: 0.01126\n",
      "Top words in document 3\n",
      "\tWord: Colt, TF-IDF: 0.01367\n",
      "\tWord: Magnum, TF-IDF: 0.01367\n",
      "\tWord: revolver, TF-IDF: 0.01367\n",
      "{'The': 0.0, 'Colt': 0.013667363194657226, 'Python': -0.009697148509610592, 'is': -0.006464765673073728, 'a': -0.006464765673073728, '357': 0.0045557877315524084, 'Magnum': 0.013667363194657226, 'caliber': 0.0045557877315524084, 'revolver': 0.013667363194657226, 'formerly': 0.0045557877315524084, 'manufactured': 0.0045557877315524084, 'by': 0.0, \"'s\": 0.009111575463104817, 'Manufacturing': 0.0045557877315524084, 'Company': 0.0045557877315524084, 'of': -0.003232382836536864, 'Hartford': 0.0045557877315524084, 'Connecticut': 0.0045557877315524084, 'It': 0.0, 'sometimes': 0.0045557877315524084, 'referred': 0.0045557877315524084, 'to': 0.0045557877315524084, 'as': 0.0, 'Combat': 0.0045557877315524084, '1': 0.0045557877315524084, 'was': 0.0, 'first': 0.0045557877315524084, 'introduced': 0.0045557877315524084, 'in': -0.003232382836536864, '1955': 0.0045557877315524084, 'the': -0.019394297019221185, 'same': 0.0045557877315524084, 'year': 0.0045557877315524084, 'Smith': 0.0045557877315524084, 'amp': 0.0045557877315524084, 'Wesson': 0.0045557877315524084, 'M29': 0.0045557877315524084, '44': 0.0045557877315524084, 'now': 0.0045557877315524084, 'discontinued': 0.0045557877315524084, 'targeted': 0.0045557877315524084, 'premium': 0.0045557877315524084, 'market': 0.0045557877315524084, 'segment': 0.0045557877315524084, 'Some': 0.0045557877315524084, 'firearm': 0.0045557877315524084, 'collectors': 0.0045557877315524084, 'and': -0.006464765673073728, 'writers': 0.0045557877315524084, 'such': 0.0045557877315524084, 'Jeff': 0.0045557877315524084, 'Cooper': 0.0045557877315524084, 'Ian': 0.0045557877315524084, 'V': 0.0045557877315524084, 'Hogg': 0.0045557877315524084, 'Chuck': 0.0045557877315524084, 'Hawks': 0.0045557877315524084, 'Leroy': 0.0045557877315524084, 'Thompson': 0.0045557877315524084, 'Renee': 0.0045557877315524084, 'Smeets': 0.0045557877315524084, 'Martin': 0.0045557877315524084, 'Dougherty': 0.0045557877315524084, 'have': 0.0045557877315524084, 'described': 0.0045557877315524084, 'finest': 0.0045557877315524084, 'production': 0.0045557877315524084, 'ever': 0.0045557877315524084, 'made': 0.0045557877315524084}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from __future__ import division, unicode_literals\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    \n",
    "    return tf(word, blob) * idf(word, bloblist)\n",
    "\n",
    "document1 = tb(\"\"\"Python is a 2000 made-for-TV horror movie directed by Richard Clabaugh. The film features several cult favorite actors, including William Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy, Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean Whalen. The film concerns a genetically engineered snake, a python, that escapes and unleashes itself on a small town. It includes the classic final girl scenario evident in films like Friday the 13th. It was filmed in Los Angeles, California and Malibu, California. Python was followed by two sequels: Python II (2002) and Boa vs. Python (2004), both also made-for-TV films.\"\"\")\n",
    "document2 = tb(\"\"\"Python, from the Greek word, is a genus of nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are recognised.[2] A member of this genus, P. reticulatus, is among the longest snakes known.\"\"\")\n",
    "document3 = tb(\"\"\"The Colt Python is a .357 Magnum caliber revolver formerly manufactured by Colt's Manufacturing Company of Hartford, Connecticut.  It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued Colt Python targeted the premium revolver market segment. Some firearm collectors and writers such as Jeff Cooper, Ian V. Hogg, Chuck Hawks, Leroy Thompson, Renee Smeets and Martin Dougherty have described the Python as the finest production revolver ever made.\"\"\")\n",
    "\n",
    "\n",
    "bloblist = [document1, document2, document3]\n",
    "\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words} # 計算該文字的tf-idf值\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
    "\n",
    "print(scores)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d9a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

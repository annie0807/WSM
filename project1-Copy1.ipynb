{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "efc048e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from Parser import Parser\n",
    "import util\n",
    "from tfidf import *\n",
    "import glob, os\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from __future__ import division, unicode_literals\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "class VectorSpace:\n",
    "    \"\"\" A algebraic model for representing text documents as vectors of identifiers. \n",
    "    A document is represented as a vector. Each dimension of the vector corresponds to a \n",
    "    separate term. If a term occurs in the document, then the value in the vector is non-zero.\n",
    "    \"\"\"\n",
    "\n",
    "    #Collection of document term vectors\n",
    "    documentVectors = []\n",
    "\n",
    "    #Mapping of vector index to keyword\n",
    "    vectorKeywordIndex=[]\n",
    "\n",
    "    #Tidies terms\n",
    "    parser=None\n",
    "    \n",
    "    def __init__(self, documents=[], vectorMode = 'tf'):\n",
    "        self.documentVectors=[]\n",
    "        self.parser = Parser()\n",
    "        self.BlobList = self.getBlobList(documents)\n",
    "        self.vectorMode = vectorMode\n",
    "        if(len(documents)>0):\n",
    "            self.build(documents)\n",
    "    \n",
    "    def getBlobList(self, documents): \n",
    "        bloblist = []\n",
    "        for doc in documents:\n",
    "            wordList = self.parser.tokenise(doc)\n",
    "            wordList = self.parser.removeStopWords(wordList)\n",
    "            bloblist.append(tb(\" \".join(wordList)))\n",
    "        return bloblist\n",
    "            \n",
    "    def build(self,documents):\n",
    "        \"\"\" Create the vector space for the passed document strings \"\"\"\n",
    "        self.vectorKeywordIndex = self.getVectorKeywordIndex(documents)\n",
    "        self.documentVectors = [self.makeVector(document, self.vectorMode) for document in tqdm(documents)]\n",
    "\n",
    "        #print(self.vectorKeywordIndex)\n",
    "        #print(self.documentVectors)\n",
    "\n",
    "\n",
    "    def getVectorKeywordIndex(self, documentList):\n",
    "        \"\"\" create the keyword associated to the position of the elements within the document vectors \"\"\"\n",
    "\n",
    "        #Mapped documents into a single word string\t\n",
    "        vocabularyString = \" \".join(documentList)\n",
    "\n",
    "        vocabularyList = self.parser.tokenise(vocabularyString)\n",
    "        #print(vocabularyList)\n",
    "        #print(vocabularyString)\n",
    "        #Remove common words which have no search value\n",
    "        vocabularyList = self.parser.removeStopWords(vocabularyList)\n",
    "        #print(vocabularyList)\n",
    "        uniqueVocabularyList = util.removeDuplicates(vocabularyList)\n",
    "        #print(uniqueVocabularyList)\n",
    "        vectorIndex={}\n",
    "        offset=0\n",
    "        #Associate a position with the keywords which maps to the dimension on the vector used to represent this word\n",
    "        for word in uniqueVocabularyList:\n",
    "            vectorIndex[word]=offset\n",
    "            offset+=1\n",
    "        return vectorIndex  #(keyword:position)\n",
    "\n",
    "\n",
    "    def makeVector(self, wordString, mode):\n",
    "        \"\"\" @pre: unique(vectorIndex) \"\"\"\n",
    "        #Initialise vector with 0's\n",
    "        vector = [0] * len(self.vectorKeywordIndex)\n",
    "        wordList = self.parser.tokenise(wordString)\n",
    "        wordList = self.parser.removeStopWords(wordList)\n",
    "        tbString = tb(\" \".join(wordList))\n",
    "        #print(wordList)\n",
    "        if mode == 'tf':\n",
    "            for word in list(set(wordList)):\n",
    "                vector[self.vectorKeywordIndex[word]] = tf(word, tbString) #Use simple Term Count Model\n",
    "            return vector \n",
    "        \n",
    "        if mode == 'tf-idf':\n",
    "            #print('bloblist:', self.BlobList)\n",
    "            for word in list(set(wordList)):\n",
    "                #print('word',word)\n",
    "                vector[self.vectorKeywordIndex[word]] =  tfidf(word, tbString , self.BlobList) \n",
    "                #print('word:',word, 'idf:', idf(word, self.BlobList),  )\n",
    "                #print('word:', word, 'tf:', tf(word, tbString))\n",
    "            return vector\n",
    "\n",
    "    def buildQueryVector(self, termList):\n",
    "        \"\"\" convert query string into a term vector \"\"\"\n",
    "        #print(termList)\n",
    "        #print(self.vectorMode)\n",
    "        query = self.makeVector(\" \".join(termList), self.vectorMode)\n",
    "        return query\n",
    "\n",
    "    def related(self,documentId):\n",
    "        \"\"\" find documents that are related to the document indexed by passed Id within the document Vectors\"\"\"\n",
    "        ratings = [util.cosine(self.documentVectors[documentId], documentVector) for documentVector in self.documentVectors]\n",
    "        #ratings.sort(reverse=True)\n",
    "        return ratings\n",
    "    \n",
    "    def search(self,searchList, mode = 'cos'):\n",
    "        \"\"\" search for documents that match based on a list of terms \"\"\"\n",
    "        #print(searchList)\n",
    "        if type(searchList[0]) == str:\n",
    "            queryVector = self.buildQueryVector(searchList)\n",
    "            #print(queryVector)\n",
    "        else:\n",
    "            queryVector = searchList\n",
    "        if mode == 'cos':\n",
    "            ratings = [util.cosine(queryVector, documentVector) for documentVector in tqdm(self.documentVectors)]\n",
    "        #ratings.sort(reverse=True)\n",
    "            return ratings\n",
    "        if mode == 'eucli':\n",
    "            ratings = [util.Euclidean(queryVector, documentVector) for documentVector in tqdm(self.documentVectors)]\n",
    "            return ratings\n",
    "\n",
    "    def printresult(self,searchlist, files, mode='cos'):\n",
    "        scoreList = self.search(searchlist, mode = mode)\n",
    "        for i in np.flip(np.argsort(scoreList))[:10]:\n",
    "            print( 'document:' , files[i],'score:', scoreList[i])\n",
    "        return np.flip(np.argsort(scoreList))[:10]\n",
    "            \n",
    "    def getFeedbackVector(self, searchList, top10results ,mode = 'cos'):\n",
    "        queryVector = self.buildQueryVector(searchList)\n",
    "        feedbackVector = self.buildQueryVector(top10results)\n",
    "        queryArray = np.array(queryVector)\n",
    "        feedbackArray = np.array(feedbackVector)\n",
    "        newQueryVector = list(queryArray + 0.5 * feedbackArray)\n",
    "        return newQueryVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21bd51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "files = []\n",
    "for file in os.listdir(\"./EnglishNews/EnglishNews\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        filename = os.path.join(\"./EnglishNews/EnglishNews\", file)\n",
    "        files.append(file[:-4])\n",
    "        with open(filename, encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            doc = ' '.join(lines)\n",
    "            doc1 = doc.replace(\"\\n\", \"\")\n",
    "            documents.append(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e462a467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7034/7034 [00:12<00:00, 576.27it/s]\n"
     ]
    }
   ],
   "source": [
    "vectorSpace_tf = VectorSpace(documents, 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3616aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"Trump Biden Taiwan China\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a96060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency (TF) Weighting + Cosine Similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7034/7034 [01:32<00:00, 76.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document: News119356 score: 0.5163977794943223\n",
      "document: News123256 score: 0.5163977794943223\n",
      "document: News120265 score: 0.46852128566581813\n",
      "document: News108578 score: 0.46852128566581813\n",
      "document: News103117 score: 0.42874646285627205\n",
      "document: News115594 score: 0.42640143271122094\n",
      "document: News112667 score: 0.4008918628686366\n",
      "document: News122919 score: 0.4003203845127178\n",
      "document: News111959 score: 0.39528470752104733\n",
      "document: News101763 score: 0.39528470752104733\n",
      "Term Frequency (TF) Weighting + Euclidean Distance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7034/7034 [00:57<00:00, 123.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document: News110871 score: 0.6708203932499369\n",
      "document: News108482 score: 0.6708203932499369\n",
      "document: News108964 score: 0.6708203932499369\n",
      "document: News110141 score: 0.6708203932499369\n",
      "document: News108940 score: 0.6708203932499369\n",
      "document: News111696 score: 0.6708203932499369\n",
      "document: News107883 score: 0.6614378277661477\n",
      "document: News108270 score: 0.6454972243679028\n",
      "document: News107832 score: 0.6454972243679028\n",
      "document: News110401 score: 0.6454972243679028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Term Frequency (TF) Weighting + Cosine Similarity')\n",
    "rank_cos = vectorSpace_tf.printresult(query,files,'cos')\n",
    "print('Term Frequency (TF) Weighting + Euclidean Distance')\n",
    "rank_euc = vectorSpace_tf.printresult(query,files, mode = 'eucli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d937a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency (TF) Weighting + Cosine Similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7034/7034 [02:11<00:00, 53.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document: News123256 score: 0.5647630786445903\n",
      "document: News119356 score: 0.5647630786445903\n",
      "document: News120265 score: 0.5119909520356302\n",
      "document: News108578 score: 0.5119909520356302\n",
      "document: News103117 score: 0.4548918488685839\n",
      "document: News115594 score: 0.45453885681717704\n",
      "document: News115859 score: 0.4468905173012529\n",
      "document: News111959 score: 0.4468905173012529\n",
      "document: News101763 score: 0.4468905173012529\n",
      "document: News119746 score: 0.4468905173012529\n",
      "Term Frequency (TF) Weighting + Euclidean Distance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7034/7034 [00:51<00:00, 135.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document: News108964 score: 0.7039485425409744\n",
      "document: News108482 score: 0.7039485425409744\n",
      "document: News111696 score: 0.7039485425409744\n",
      "document: News110141 score: 0.7039485425409744\n",
      "document: News110871 score: 0.7031339977133836\n",
      "document: News108940 score: 0.7031339977133836\n",
      "document: News107883 score: 0.6950133455880988\n",
      "document: News110401 score: 0.6798604395111019\n",
      "document: News107832 score: 0.6798604395111019\n",
      "document: News108270 score: 0.6798604395111019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2221, 2102, 2957, 2515, 2727, 2214, 1958, 2577, 1949, 2053],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "feedBack = []\n",
    "for i in rank_cos:\n",
    "    feedBack.append(documents[i])\n",
    "    feedback = [\" \".join(feedBack)]\n",
    "\n",
    "print('Term Frequency (TF) Weighting + Cosine Similarity')    \n",
    "newquery = vectorSpace_tf.getFeedbackVector(query, feedback, 'cos')\n",
    "vectorSpace_tf.printresult(newquery,files,'cos')\n",
    "\n",
    "print('Term Frequency (TF) Weighting + Euclidean Distance')\n",
    "vectorSpace_tf.printresult(newquery,files, mode = 'eucli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70877b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487d207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b7b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f9f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60066a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.flip(np.argsort(scoreList))[:10]:\n",
    "    print( 'document:' , files[i],'score:', scoreList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d095baad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7034/7034 [25:01<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Weighting + Cosine Similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7034/7034 [01:40<00:00, 69.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document: News103134 score: 0.46434426041191357\n",
      "document: News103767 score: 0.44893938649720727\n",
      "document: News104913 score: 0.41023815627691995\n",
      "document: News108813 score: 0.41023815627691995\n",
      "document: News116613 score: 0.41023815627691995\n",
      "document: News104914 score: 0.3887053364880285\n",
      "document: News112714 score: 0.3887053364880285\n",
      "document: News101014 score: 0.3887053364880285\n",
      "document: News116634 score: 0.3824226011372703\n",
      "document: News103728 score: 0.36096936067594315\n",
      "TF-IDF Weighting + Euclidean Distance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7034/7034 [00:34<00:00, 203.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document: News111696 score: 3.2621792687215017\n",
      "document: News107883 score: 3.188692831038035\n",
      "document: News108964 score: 3.1789584035989207\n",
      "document: News122771 score: 3.166603585904829\n",
      "document: News110747 score: 3.12086863322629\n",
      "document: News110141 score: 3.0968146116076833\n",
      "document: News108940 score: 3.0117984545040595\n",
      "document: News108482 score: 2.955387623457143\n",
      "document: News107804 score: 2.9483446300958107\n",
      "document: News108270 score: 2.9218838035101777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vectorSpace_tfidf = VectorSpace(documents ,'tf-idf')\n",
    "print('TF-IDF Weighting + Cosine Similarity')\n",
    "vectorSpace_tfidf.printresult([\"Trump Biden Taiwan China\"],files)\n",
    "print('TF-IDF Weighting + Euclidean Distance')\n",
    "vectorSpace_tfidf.printresult([\"Trump Biden Taiwan China\"],files, mode = 'eucli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d9a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b0f0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22152a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: nan document: I haven't got a hat.\n",
      "score: nan document: Dogs and cats make good pets.\n",
      "score: nan document: A cat is a fine pet ponies.\n",
      "score: nan document: The cat cat in the hat disabled\n",
      "score: 1.2345525572306575 document: Dogs and cats make good pets.\n",
      "score: 1.0216002166437488 document: A cat is a fine pet ponies.\n",
      "score: 0.7504758415354574 document: The cat cat in the hat disabled\n",
      "score: 0.28768207245178085 document: I haven't got a hat.\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "\n",
    "documents = [\"The cat cat in the hat disabled\",\n",
    "                 \"A cat is a fine pet ponies.\",\n",
    "                 \"Dogs and cats make good pets.\",\n",
    "                 \"I haven't got a hat.\"]\n",
    "\n",
    "vectorSpace = VectorSpace(documents, 'tf-idf')  # vectorSpace(documents, vectorMode = 'tf' or 'tf-idf') (default is 'tf')\n",
    "\n",
    "\n",
    "#print(vectorSpace.vectorKeywordIndex)\n",
    "\n",
    "#print(vectorSpace.documentVectors)\n",
    "\n",
    "#print(vectorSpace.related(1))\n",
    "\n",
    "#print(vectorSpace.search([\"cat\"]))   \n",
    "\n",
    "vectorSpace.printresult([\"cat\"])\n",
    "vectorSpace.printresult([\"cat\"], mode = 'eucli')  # mode = 'cos' or 'eucli' (default is 'cos')\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461279fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2  0  2 -1  3]\n",
      "[4 0 4 1 9]\n",
      "[2. 0. 2. 1. 3.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([3, 2, 1, 5, 2])\n",
    "print(x-y)\n",
    "print((x-y) ** 2)\n",
    "print(((x-y) ** 2) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f9301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
